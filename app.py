import os
import torch
import torch.nn as nn
from torchvision import transforms, models
from torchvision.models import resnet50
from PIL import Image
import io
import base64
from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import numpy as np
import requests
import json
from datetime import datetime

app = Flask(__name__)
CORS(app)  # Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ù…Ø´Ú©Ù„ CORS

# ============ ØªÙ†Ø¸ÛŒÙ…Ø§Øª ============
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ Server starting on {DEVICE}")

# âœ… API Key OpenRouter - Ø±Ø§ÛŒÚ¯Ø§Ù†
OPENROUTER_API_KEY = "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"

# Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ
CLASS_NAMES = ['normal', 'crohn', 'ulcerative-colitis']
CLASS_NAMES_FA = {
    'normal': 'Ù†Ø±Ù…Ø§Ù„',
    'crohn': 'Ú©Ø±ÙˆÙ†',
    'ulcerative-colitis': 'Ú©ÙˆÙ„ÛŒØª Ø§ÙˆÙ„Ø³Ø±Ø§ØªÛŒÙˆ'
}

# ============ Ù…Ø¯Ù„ ResNet50 ============
class IBDResNet(nn.Module):
    def __init__(self, num_classes=3):
        super(IBDResNet, self).__init__()
        self.backbone = resnet50(weights=None)
        num_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Identity()
        
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(num_features, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        features = self.backbone(x)
        return self.classifier(features)

# ============ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ ============
model = None
model_loaded = False

try:
    model_path = 'models/final_ibd_model.pth'
    if os.path.exists(model_path):
        print(f"ğŸ“‚ ÙØ§ÛŒÙ„ Ù…Ø¯Ù„ Ù¾ÛŒØ¯Ø§ Ø´Ø¯: {model_path}")
        model = IBDResNet(num_classes=3)
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
        checkpoint = torch.load(model_path, map_location=DEVICE)
        
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        
        model = model.to(DEVICE)
        model.eval()
        model_loaded = True
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø­Ø¬Ù… Ù…Ø¯Ù„
        model_size = os.path.getsize(model_path) / (1024 * 1024)
        print(f"âœ… Ù…Ø¯Ù„ ResNet50 Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù„ÙˆØ¯ Ø´Ø¯")
        print(f"   - Device: {DEVICE}")
        print(f"   - Ø­Ø¬Ù… ÙØ§ÛŒÙ„: {model_size:.1f} MB")
        print(f"   - Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§: {CLASS_NAMES}")
    else:
        print(f"âŒ ÙØ§ÛŒÙ„ Ù…Ø¯Ù„ ÛŒØ§ÙØª Ù†Ø´Ø¯: {model_path}")
        print(f"   - Ù…Ø³ÛŒØ± Ø¬Ø§Ø±ÛŒ: {os.getcwd()}")
        print(f"   - Ù…Ø­ØªÙˆÛŒØ§Øª Ù¾ÙˆØ´Ù‡ models: {os.listdir('models') if os.path.exists('models') else 'Ù¾ÙˆØ´Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯'}")
except Exception as e:
    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù„ÙˆØ¯ Ù…Ø¯Ù„: {e}")
    import traceback
    traceback.print_exc()

# ============ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ± ============
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# ============ ØªØ§Ø¨Ø¹ Ø¬Ù…Ù„Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø§ OpenRouter (Ø±Ø§ÛŒÚ¯Ø§Ù†) ============
def format_with_llm(disease_fa, confidence):
    """
    ØªØ¨Ø¯ÛŒÙ„ Ù†ØªÛŒØ¬Ù‡ ØªØ´Ø®ÛŒØµ Ø¨Ù‡ Ø¬Ù…Ù„Ù‡ Ø±ÙˆØ§Ù† Ø¨Ø§ Llama 3.2 Vision
    Ù…Ø¯Ù„: meta-llama/llama-3.2-11b-vision-instruct:free (Ú©Ø§Ù…Ù„Ø§Ù‹ Ø±Ø§ÛŒÚ¯Ø§Ù†)
    """
    
    # Ù¾Ø±Ø§Ù…Ù¾Øª Ø³Ø§Ø¯Ù‡ Ùˆ Ù…Ø³ØªÙ‚ÛŒÙ…
    prompt = f"""Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù¾Ø²Ø´Ú©ÛŒØŒ Ø§ÛŒÙ† Ù†ØªÛŒØ¬Ù‡ ØªØ´Ø®ÛŒØµ Ø±Ø§ Ø¨Ù‡ ÛŒÚ© Ø¬Ù…Ù„Ù‡ Ø³Ø§Ø¯Ù‡ Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†:

ØªØ´Ø®ÛŒØµ: {disease_fa}
Ø¯Ø±ØµØ¯ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†: {confidence:.1f}%

ÙÙ‚Ø· ÛŒÚ© Ø¬Ù…Ù„Ù‡ Ú©ÙˆØªØ§Ù‡ Ùˆ Ø±ÙˆØ§Ù† Ø¨Ù†ÙˆÛŒØ³ØŒ Ø¨Ø¯ÙˆÙ† ØªÙˆØ¶ÛŒØ­ Ø§Ø¶Ø§ÙÙ‡."""

    try:
        print(f"ğŸŸ¡ Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ OpenRouter...")
        
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {OPENROUTER_API_KEY}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://crohn-1.onrender.com",
                "X-Title": "Crohn IBD Detector"
            },
            json={
                "model": "meta-llama/llama-3.2-11b-vision-instruct:free",  # âœ… Ù…Ø¯Ù„ Ø±Ø§ÛŒÚ¯Ø§Ù†
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": 60,
                "temperature": 0.3
            },
            timeout=10
        )
        
        if response.status_code == 200:
            result = response.json()
            llm_response = result["choices"][0]["message"]["content"].strip()
            print(f"âœ… Ù¾Ø§Ø³Ø® OpenRouter: {llm_response}")
            return llm_response
        else:
            print(f"âš ï¸ OpenRouter Ø®Ø·Ø§: {response.status_code}")
            print(f"   Ù¾Ø§Ø³Ø®: {response.text[:200]}")
            return None
            
    except Exception as e:
        print(f"âš ï¸ OpenRouter Ø®Ø·Ø§: {e}")
        return None

# ============ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ API ============

@app.route('/')
def index():
    """ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ Ú†Øª Ø¨Ø§Øª"""
    return render_template('chat.html')

@app.route('/health')
def health():
    """Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª Ø³Ø±ÙˆØ±"""
    return jsonify({
        'status': 'healthy',
        'model_loaded': model_loaded,
        'device': str(DEVICE),
        'timestamp': datetime.now().isoformat(),
        'llm_ready': OPENROUTER_API_KEY is not None and OPENROUTER_API_KEY.startswith('sk-or-')
    })

@app.route('/api/predict', methods=['POST'])
def predict():
    """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ØªØµÙˆÛŒØ± Ø¢Ù†Ø¯ÙˆØ³Ú©ÙˆÙ¾ÛŒ"""
    
    # Ú†Ú© Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„
    if not model_loaded:
        return jsonify({
            'success': False,
            'error': 'Ù…Ø¯Ù„ Ù‡Ù†ÙˆØ² Ù„ÙˆØ¯ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª. Ù„Ø·ÙØ§Ù‹ Ú†Ù†Ø¯ Ù„Ø­Ø¸Ù‡ Ø¯ÛŒÚ¯Ø± ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.'
        }), 503
    
    try:
        # Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡
        data = request.json
        if not data:
            return jsonify({'success': False, 'error': 'Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø®Ø§Ù„ÛŒ Ø§Ø³Øª'}), 400
        
        image_data = data.get('image')
        if not image_data:
            return jsonify({'success': False, 'error': 'Ø¹Ú©Ø³ÛŒ Ø§Ø±Ø³Ø§Ù„ Ù†Ø´Ø¯Ù‡'}), 400
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ±
        if ',' in image_data:
            image_data = image_data.split(',')[1]
        
        try:
            image_bytes = base64.b64decode(image_data)
            image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
        except Exception as e:
            return jsonify({'success': False, 'error': 'ÙØ±Ù…Øª ØªØµÙˆÛŒØ± Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª'}), 400
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ ResNet50
        input_tensor = transform(image).unsqueeze(0).to(DEVICE)
        
        with torch.no_grad():
            outputs = model(input_tensor)
            probabilities = torch.nn.functional.softmax(outputs, dim=1)
            confidence, prediction = torch.max(probabilities, 1)
        
        class_idx = prediction.item()
        class_name = CLASS_NAMES[class_idx]
        class_name_fa = CLASS_NAMES_FA[class_name]
        confidence_score = confidence.item()
        
        print(f"âœ… Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ: {class_name_fa} | Ø§Ø·Ù…ÛŒÙ†Ø§Ù†: {confidence_score:.1%}")
        
        # ============ Ø¬Ù…Ù„Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø§ OpenRouter ============
        llm_response = format_with_llm(class_name_fa, confidence_score * 100)
        
        # Ø§Ú¯Ø± OpenRouter Ø¬ÙˆØ§Ø¨ Ù†Ø¯Ø§Ø¯ØŒ Ø§Ø² Ø¬Ù…Ù„Ù‡ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
        if not llm_response:
            if class_name == 'normal':
                llm_response = f"âœ… ØªØµÙˆÛŒØ± Ø¢Ù†Ø¯ÙˆØ³Ú©ÙˆÙ¾ÛŒ Ø´Ù…Ø§ Ù†Ø±Ù…Ø§Ù„ Ø§Ø³Øª. Ø¨Ø§ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† {confidence_score*100:.1f}% Ù‡ÛŒÚ† Ù†Ø´Ø§Ù†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø§Ù„ØªÙ‡Ø§Ø¨ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù†Ø´Ø¯."
            elif class_name == 'crohn':
                llm_response = f"âš ï¸ ÛŒØ§ÙØªÙ‡â€ŒÙ‡Ø§ÛŒ ØªØµÙˆÛŒØ± Ø¨Ø§ Ø¨ÛŒÙ…Ø§Ø±ÛŒ Ú©Ø±ÙˆÙ† Ø³Ø§Ø²Ú¯Ø§Ø± Ø§Ø³Øª. (Ø¯Ø±ØµØ¯ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†: {confidence_score*100:.1f}%)"
            else:
                llm_response = f"âš ï¸ ÛŒØ§ÙØªÙ‡â€ŒÙ‡Ø§ÛŒ ØªØµÙˆÛŒØ± Ø¨Ø§ Ú©ÙˆÙ„ÛŒØª Ø§ÙˆÙ„Ø³Ø±Ø§ØªÛŒÙˆ Ø³Ø§Ø²Ú¯Ø§Ø± Ø§Ø³Øª. (Ø¯Ø±ØµØ¯ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†: {confidence_score*100:.1f}%)"
            
            print(f"âšª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¬Ù…Ù„Ù‡ Ù¾ÛŒØ´â€ŒÙØ±Ø¶")
        
        # Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù†ØªÛŒØ¬Ù‡
        return jsonify({
            'success': True,
            'class': class_name,
            'class_fa': class_name_fa,
            'confidence': float(confidence_score),
            'confidence_percent': f"{confidence_score*100:.1f}%",
            'explanation': llm_response,
            'llm_used': llm_response is not None and not llm_response.startswith('âœ…') and not llm_response.startswith('âš ï¸')
        })
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'success': False,
            'error': f'Ø®Ø·Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ Ø³Ø±ÙˆØ±: {str(e)}'
        }), 500

@app.route('/api/test', methods=['GET'])
def test():
    """ØªØ³Øª Ø³Ø§Ø¯Ù‡ API"""
    return jsonify({
        'success': True,
        'message': 'Ø³Ø±ÙˆØ± Crohn IBD Detector ÙØ¹Ø§Ù„ Ø§Ø³Øª',
        'model_loaded': model_loaded,
        'device': str(DEVICE),
        'llm_configured': OPENROUTER_API_KEY is not None and OPENROUTER_API_KEY.startswith('sk-or-')
    })

@app.route('/api/test-llm', methods=['GET'])
def test_llm():
    """ØªØ³Øª Ø§ØªØµØ§Ù„ Ø¨Ù‡ OpenRouter"""
    try:
        test_response = format_with_llm("Ú©Ø±ÙˆÙ†", 85.5)
        if test_response:
            return jsonify({
                'success': True,
                'message': 'Ø§ØªØµØ§Ù„ Ø¨Ù‡ OpenRouter Ø¨Ø±Ù‚Ø±Ø§Ø± Ø§Ø³Øª',
                'response': test_response
            })
        else:
            return jsonify({
                'success': False,
                'message': 'OpenRouter Ù¾Ø§Ø³Ø® Ù†Ø¯Ø§Ø¯'
            }), 503
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# ============ Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ ============
if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    print(f"ğŸŒ Server running on port {port}")
    print(f"ğŸ“ OpenRouter API Key: {'âœ… ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯Ù‡' if OPENROUTER_API_KEY.startswith('sk-or-') else 'âŒ ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡'}")
    print(f"ğŸ§  Ù…Ø¯Ù„: {'âœ… Ù„ÙˆØ¯ Ø´Ø¯' if model_loaded else 'âŒ Ù„ÙˆØ¯ Ù†Ø´Ø¯'}")
    print("="*50)
    app.run(host='0.0.0.0', port=port)
